TOKENIZERS_PARALLELISM=true
# prevent huge fragmentation spikes
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
AWS_PROFILE_NAME=
AWS_ROLE_ARN=
AWS_SESSION_NAME=
HF_USERNAME=
HF_TOKEN=